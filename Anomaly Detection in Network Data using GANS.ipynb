{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Network Data using GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third lab of this series!\n",
    "\n",
    "In the previous labs, we tried our hand at supervised and unsupervised anomaly detection using XGBoost and Deep Autoencoders on the KDD-99 network intrusion dataset.\n",
    "\n",
    "We addressed the issue of unlabelled training data through the use of Deep Autoencoders in the second lab. However, unsupervised methods such as PCA and Autoencoders tend to be effective only on highly correlated data such as the KDD dataset, and these algorithms might also require the data to follow a Gaussian Distribution.\n",
    "\n",
    "> \"Adversarial training (also called GAN for Generative Adversarial Networks), and the variations that are now being proposed, is the most interesting idea in the last 10 years in ML, in my opinion.\".\n",
    "> Yann LeCun, 2016.\n",
    "\n",
    "What do GANs bring to the table and how are they different from Deep Autoencoders?\n",
    "\n",
    "GANs are generative models that generate samples similar to the training dataset by learning the true data distribution. So instead of compressing the input into a latent space and classifying the test samples based on the reconstruction error, we actually train a classifier that outputs a probability score of a sample being Normal or Anomalous. As we will see later in the lab, this has positioned GANs as very attaractive unsupervised learning techniques.\n",
    "\n",
    "GANs can be pretty tough to train and improving their stability is an active area of research today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "#Import data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing ML/DL libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, label\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc,precision_recall_fscore_support, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix,accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras import layer\n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, LeakyReLU, Dense, Reshape, Flatten, Activation \n",
    "from tensorflow.keras.layers import Dropout, multiply, GaussianNoise, MaxPooling2D, concatenate\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the same KDD-99 dataset that we used in the previous labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pickled file\n",
    "\n",
    "filename = data_path + 'preprocessed_data_full.pkl'\n",
    "input_file = open(filename,'rb')\n",
    "preprocessed_data = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Lab 2, we will split the pickled data into vectors and assign them to the label encoder `le`, training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in preprocessed_data:\n",
    "    print(key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessed_data['le']\n",
    "x_train = preprocessed_data['x_train']\n",
    "y_train = preprocessed_data['y_train']\n",
    "x_test = preprocessed_data['x_test']\n",
    "y_test = preprocessed_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider the binary classification problem. Similar to previous labs, *Normal* data points will be labeled as '0' and *Anomalous* points will be labeled as '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the class number for Normal entries \n",
    "pd.DataFrame(le.classes_, columns = ['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to Binary\n",
    "\n",
    "y_test[y_test != 11] = 1 \n",
    "y_test[y_test == 11] = 0\n",
    "y_train[y_train != 11] = 1\n",
    "y_train[y_train == 11] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the dataset into normal and anomalous data. We will need to do this in order to be able to train GANs to generate Normal packets only and then predict the anomaly based on the Discriminator output. The details regarding this will be covered later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting only Normal Network packets in our training set\n",
    "\n",
    "temp_df = x_train.copy()\n",
    "temp_df['label'] = y_train\n",
    "temp_df = temp_df.loc[temp_df['label'] == 0]\n",
    "temp_df = temp_df.drop('label', axis = 1)\n",
    "x_train = temp_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Lab 2, we scale the input training data between 0 and 1 before feeding it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the above splits using the MinMaxScaler from the scikit learn package\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Make sure to only fit the scaler on the training data\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "#Creating dataset dictionary \n",
    "dataset = {}\n",
    "dataset['x_train'] = x_train.astype(np.float32)\n",
    "dataset['y_train'] = y_train.astype(np.float32)\n",
    "dataset['x_test']  = x_test.astype(np.float32)\n",
    "dataset['y_test']  = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the datasets** :\n",
    "- The Training set consists of only normal network packets.\n",
    "- The Testing set comprises a small number of anomalous network packets of about 1%, reflecting what we see in the real world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many anomalies are in our Testing set\n",
    "print('Number of Normal Network packets in the Training set:', x_train.shape[0])\n",
    "print('Number of Normal Network packets in the Testing set:', collections.Counter(y_test)[0])\n",
    "print('Number of Anomalous Network packets in the Testing set:', collections.Counter(y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GAN) were introduced by Ian Goodfellow in [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), Goodfellow, 2014.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://drive.google.com/uc?id=14f7ZwXxyigpwx7bIVFxAOSb_aV28bFav\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "<br>\n",
    "\n",
    "\n",
    "The GAN consists of two networks namely: \n",
    "- The generator `G` that produces fake samples\n",
    "- The discriminator `D` that that receives samples from both `G` and the dataset.\n",
    "\n",
    "During Training the two networks have competing goals. The generator tries to fool the discriminator by outputting values that resemble real data and the discriminator tries to become better at distinguishing between the real and fake data.\n",
    "\n",
    "Mathematically, this means that the Generator's weights are optimized to maximize the probability that fake data is classified as belonging to the real data. The discriminators's weights are optimized to maximize the probability that the real input data is classified as real while minimizing the probability of fake input data being classified as real.\n",
    "\n",
    "Optimality is reached when the generator produces an output that the disciminator cannot concretely label as real or fake and this, happens when either of the networks cannot improve anymore.\n",
    "\n",
    "<img src=./images/gan-optimality.PNG alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "The first part of the above equation reflects the log probability of the discriminator predicting that the input sample is genuine and the second half reflects the probability of the Discriminator predicting that the Generator's output is not genuine.\n",
    "\n",
    "In this lab, we will be train our GAN on normal network packets. The generator inputs noise and as training progresses the GAN learns the mapping between these random values to the input distribution. The discriminator outputs a score of how likely the the generated output resembles the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generator Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator is used to synthesize fake data points. As shown below, it consists of 5 Dense Layers with a `tanh` activation function and uses `binary cross-entropy` for calculating the generator loss. Binary cross-entropy loss measures the performance of a two class classification model whose output is a probability value between 0 and 1. A perfect model would have a loss of 0.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Igvb1fdAQqRFn9of9-EzwkxJarIgqDfJ\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Generator network\n",
    "\n",
    "def get_generator(optimizer):\n",
    "    \n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(64, input_dim=114, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(128))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(256))\n",
    "    generator.add(Activation('tanh'))\n",
    "    \n",
    "    generator.add(Dense(256))\n",
    "    generator.add(Activation('tanh'))\n",
    "       \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(Activation('tanh'))\n",
    "   \n",
    "    generator.add(Dense(114, activation='tanh'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1kaq-GZjHpf131v-fevLCQFCGm5Ala29z\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "The Discriminator basically outputs the score of a sample belonging to the real dataset or the synthetic dataset. It consists of 6 dense layers-each followed by a dropout layer to help prevent overfitting. The sigmoid activation function is applied to the final layer to obtain a value in the range 0 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Defining the Discriminator network\n",
    "\n",
    "def get_discriminator(optimizer):\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(256, input_dim=114, kernel_initializer=initializers.glorot_normal(seed=42)))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "       \n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(Activation('relu'))\n",
    "    discriminator.add(Dropout(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1))\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "   \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we built the individual networks that constitute the GAN. Let us now stich them together by using the generator and discriminator as individual layers of the overall generative adversarial network. We set the field `discriminator.trainable = False` as we want to alter the weights of only one of them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, generator, optimizer,input_dim=114):\n",
    "\n",
    "    discriminator.trainable = False   \n",
    "    gan_input = Input(shape=(input_dim,))  \n",
    "    x = generator(gan_input)        \n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)    \n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Setting the Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a GAN can be quite tricky and time consuming. Refer to this <a href=\"https://github.com/soumith/ganhacks#authors\">  link </a> to get a few ideas on how you could get your models to converge faster by choosing appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "adam = Adam(lr = learning_rate,beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator first predicts on a batch of noise samples. As the generator has randomly initialized weights initially, the output of the generator at this stage is nothing but meaningless values.\n",
    "<br>\n",
    "<img src=./images/Gan-Page-04.jpg alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "<br>\n",
    "The Discriminator inputs a stack of samples - the first half of which is the output of the generator and the second half is a batch of data samples from the real dataset. We train the Discriminator on this stack with the target labels  0 (Fake) for half the stack and 1 for the second half of the stack. The result of this is that the Discrimator is able to distinguish between the Real and Fake samples.\n",
    "<br>\n",
    "<img src=./images/Gan-Page-2.jpg alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "<br>\n",
    "The weights of the discriminator are frozen by setting the trainable parameter to False.\n",
    "\n",
    "To train the Generator, We first feed it random noise and let the entire GAN output a probability with the Discriminator weights remaining frozen. As expected this value would be less than 0.5 since the Discriminator was previously set to output a value close to 0 if the input was not genuine .\n",
    "<br>\n",
    "<img src=./images/Gan-Page-3.jpg alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "<br>\n",
    "\n",
    "Now comes the trick. We tell the GAN that the expected output is 1. This results in the errors being backpropagated only to the Generator. With every sample in the batch the generator's weights are tuned such that the output of the GAN is close to 1, meaning the Generator is now learning to produce samples that resemble the real data.\n",
    "\n",
    "This process loops back to the first step for each batch in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the GAN\n",
    "x_train, y_train, x_test, y_test = dataset['x_train'], dataset['y_train'],dataset['x_test'],dataset['y_test']\n",
    "\n",
    "#Calculating the number of batches based on the batch size\n",
    "batch_count = x_train.shape[0] // batch_size\n",
    "pbar = tqdm(total=epochs * batch_count)\n",
    "gan_loss = []\n",
    "discriminator_loss = []\n",
    "\n",
    "#Inititalizing the network\n",
    "generator = get_generator(adam)\n",
    "discriminator = get_discriminator(adam)\n",
    "gan = get_gan_network(discriminator, generator, adam,input_dim=114)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):        \n",
    "    for index in range(batch_count):        \n",
    "        pbar.update(1)        \n",
    "        # Creating a random set of input noise and images\n",
    "        noise = np.random.normal(0, 1, size=[batch_size,114])\n",
    "        \n",
    "        # Generate fake samples\n",
    "        generated_images = generator.predict_on_batch(noise)\n",
    "        \n",
    "        #Obtain a batch of normal network packets\n",
    "        image_batch = x_train[index * batch_size: (index + 1) * batch_size]\n",
    "            \n",
    "        X = np.vstack((generated_images,image_batch))       \n",
    "        y_dis = np.ones(2*batch_size) \n",
    "        y_dis[:batch_size] = 0\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = True\n",
    "        d_loss= discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.uniform(0, 1, size=[batch_size, 114])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        #Record the losses\n",
    "        discriminator_loss.append(d_loss)\n",
    "        gan_loss.append(g_loss)\n",
    "        \n",
    "    print(\"Epoch %d Batch %d/%d [D loss: %f] [G loss:%f]\" % (epoch,index,batch_count, d_loss, g_loss))\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the generator and discriminator training losses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(discriminator_loss, label='Discriminator')\n",
    "plt.plot(gan_loss, label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss convergence towards the end signifies that the GAN model has reached optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the result of all the training we did?  \n",
    "\n",
    "We now have a generator that can input a random seed value and produce an output that closely resembles the data it was trained on. You can read about how this strategy was used to come up with imaginary celebrities <a href =\"https://research.nvidia.com/publication/2017-10_Progressive-Growing-of\" >here</a>\n",
    "\n",
    "The Discriminator that we trained ended up being a very powerful classifier that can tell if a sample point is representative of the true data distribution it was trained on or not and hence can be used for Anomaly Detection!\n",
    "\n",
    "Let us feed our test data into the discriminator and obtain scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "\n",
    "nr_batches_test = np.ceil(x_test.shape[0] // batch_size).astype(np.int32)\n",
    "\n",
    "results =[]\n",
    "\n",
    "for t in range(nr_batches_test +1):    \n",
    "        ran_from = t * batch_size\n",
    "        ran_to = (t + 1) * batch_size\n",
    "        image_batch = x_test[ran_from:ran_to]             \n",
    "        tmp_rslt = discriminator.predict(x=image_batch,batch_size=128,verbose=0)        \n",
    "        results = np.append(results, tmp_rslt)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the mean score for normal and anomalous samples in our test set. Ideally, we would like to see a score close to 1 for normal samples and 0 for anomalous samples. This would mean our classifier is doing well in distinguishing between the 2 classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:20,.7f}'.format\n",
    "results_df = pd.concat([pd.DataFrame(results),pd.DataFrame(y_test)], axis=1)\n",
    "results_df.columns = ['results','y_test']\n",
    "print ('Mean score for normal packets :', results_df.loc[results_df['y_test'] == 0, 'results'].mean() )\n",
    "print ('Mean score for anomalous packets :', results_df.loc[results_df['y_test'] == 1, 'results'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how exaclty do we identify our Anomalies?\n",
    "\n",
    "Although there are several ways to do this, let us use a more straight forward way for detection. Remember 1% of our test set comprised of anomalies. So, the lowest 1% of the scores should ideally constitute anomalies. Let us test our hypothesis below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the lowest 1% score\n",
    "per = np.percentile(results,1)\n",
    "y_pred = results.copy()\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "#Thresholding based on the score\n",
    "inds = (y_pred > per)\n",
    "inds_comp = (y_pred <= per)\n",
    "y_pred[inds] = 0\n",
    "y_pred[inds_comp] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy, Precision and Recall values of our model on the test set are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1,_ = precision_recall_fscore_support(y_test,y_pred,average='binary')\n",
    "print ('Accuracy Score :',accuracy_score(y_test, y_pred) )\n",
    "print ('Precision :',precision )\n",
    "print ('Recall :',recall )\n",
    "print ('F1 :',f1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a good way to understand if our predictions agree with the target labels. You've implemented one of these in both of the previous labs, so in this section, you'll get a chance to build your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print ('Confusion Matrix :')\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Greens):\n",
    "    plt.figure(figsize=(10,10),)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our ROC curve looks with the predictions. The closer the AUC is to 1, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.2f})'.format(auc_keras))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We successfully employed state of the art Generative Adversarial Networks for anomaly detection on high dimensional data such as the KDD dataset.\n",
    "- The GAN is particularly interesting  because it sets up a supervised learning problem in order to do unsupervised learning. While it generates fake data, and tries to determine if a sample is fake or real based on trivial labels, it really does not know what the different classes in the dataset are.\n",
    "- On the downside, GANs can be tough to train and suffer from convergence issues particularly because, the discriminator during training does not learn as much from the true dataset as it learns to distinguish between the probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to consider how each of the three methods actually detected anomalies. If time, take a moment to attempt to articulate each to a partner or write in a notebook or the space below. Bonus. Reflect on how each responded to the rarity of anomalies and why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li>Zenati, H., Foo, C., Lecouat, B., Manek, G. and Chandrasekhar, V. (2018). Efficient GAN-Based Anomaly Detection. [online]   Arxiv.org. Available at: https://arxiv.org/abs/1802.06222</li>\n",
    "\n",
    "<li>Ben Poole Alex Lamb Martin Arjovsky Olivier Mastropietro Vincent Dumoulin, Ishmael Belghazi and Aaron Courville. Adversarially learned inference. International Conference on Learning Representations, 2017.</li>\n",
    "\n",
    "<li>Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A.Bharath. Generative adversarial networks: An overview. In the Proceedings of IEEE Signal Processing Magazine Special Issue on Deep Learning for Visual Understanding, accepted paper,2017.</li>\n",
    "\n",
    "<li>Martin Renqiang Min Wei Cheng Cristian Lumezanu Daeki Cho Haifeng Chen Bo Zong, Qi Song.Deep autoencoding gaussian mixture model for unsupervised anomaly detection. International Conference on Learning Representations, 2018.</li>\n",
    "\n",
    "<li>Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models for anomaly detection. International Conference on Machine Learning, pp. 1100-1109, 2016.</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
